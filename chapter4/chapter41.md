# 数据处理

数据处理是数据分析，及后续数据挖掘中最重要的部分。
数据处理： 1. 可以提高数据的质量。2。能够较好支持数据分析。 

数据处理包括：
数据清洗、数据抽取、数据交换 和数据计算

## 1 数据清洗 
### 重复值的处理

1) DataFrame 中的 duplicated 方法检查是否有重复行。

```python
import numpy as np
import pandas as pd

df = pd.DataFrame({'age':pd.Series([26, 33, 78, 23, 26]),
                  'name':pd.Series(['aa', 'bb', 'cc', 'dd', 'aa'])})
print(df)
'''
   age name
0   26   aa
1   33   bb
2   78   cc
3   23   dd
4   26   aa
'''

print(df.duplicated())
'''
print(df['name'].duplicated()) # name 这一列是不是有重复；
0    False
1    False
2    False
3    False
4     True # 最后一行是重复的。
'''
df = df.drop_duplicates()
print(df)

'''
   age name
0   26   aa
1   33   bb
2   78   cc
3   23   dd
'''
```
通过去重， 可以有效减少重复数据带来数据分析的错误。

### 缺失值处理
假设有下列有缺失值矩阵

```Python
import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.randint(low=2, high=10, size=(5, 6)))
print(df)
df.iloc[2:4, 1:3] = np.nan # 该矩阵的 第1行~第2行， 第0列到第2列 改为NAN
print(df)
'''
   0  1  2  3  4  5
0  9  9  6  7  3  8
1  6  8  3  4  5  2
2  5  9  4  4  4  7
3  3  3  2  8  8  4
4  4  3  6  4  7  4
============================

   0    1    2  3  4  5
0  9  9.0  6.0  7  3  8
1  6  8.0  3.0  4  5  2
2  5  NaN  NaN  4  4  7
3  3  NaN  NaN  8  8  4
4  4  3.0  6.0  4  7  4
'''
```
在该矩阵[2:4,1:3]处有缺失值。 

我们有几种处理方法：
#### 1. 数据补齐
```Python
df = df.fillna(method='pad')
# 用前一个数值填入
df = df.fillna(method='bfill')
# 用后一个数值填入
df = df.fillna(df.mean())
# 用均值来填补

df = df.fillna(df.mean()['A列':'B列'])
# 用B列的均值来代替A列的缺失值。
```

#### 2. 数据删除
如果在海量的数据前提下， 如果通过df.isnull()来判断该数据中缺失值较少。 删除
不影响整体数据质量情况下，我们可以选择删除存在缺失值的行

```Python
df.dropna()
```

## 2. 数据抽取

数据抽取指的是抽出指定位置的数据做成新的数据。 

### 1. head(),tail() 函数  前 5 行 和后 5 行

```Python
import pandas as pd
dt = pd.read_csv(r'k:\rz.csv', sep=',')
print(dt.head())
print(dt.tail())
```
抽取该数据集中的前5条或者后5条记录。

### 2. slice()函数 指定位置的数据做成新的列。




```Python

import pandas as pd

my_data = pd.read_excel(r'd:\textfiles\i_nuc.xls', sheet_name='Sheet4')
#读取 i_nuc.xls 中表 Sheet4


my_data = my_data.fillna(method='bfill')
# 补全
my_data['电话'] = my_data['电话'].astype(str)
brands = my_data['电话'].str.slice(0, 3)
areas = my_data['电话'].str.slice(3, 7)
tells = my_data['电话'].str.slice(7, 11)
#分割

new_data = pd.concat([my_data, brands, areas, tells], axis=1)
#在表中创建新列  可以试试axis=0的结果。
print(new_data)
[output]
            学号             电话                 IP   电话    电话    电话
0   2308024241  18922254812.0      221.205.98.55  189  2225  4812
1   2308024244  13522255003.0    183.184.226.205  135  2225  5003
2   2308024251  13422259938.0      221.205.98.55  134  2225  9938
3   2308024249  18822256753.0      222.31.51.200  188  2225  6753
4   2308024219  18922253721.0       120.207.64.3  189  2225  3721
5   2308024201  13822254373.0      222.31.51.200  138  2225  4373
6   2308024347  13822254373.0      222.31.59.220  138  2225  4373
7   2308024307  13322252452.0  221.205.98.55      133  2225  2452
8   2308024326  18922257681.0     183.184.230.38  189  2225  7681
9   2308024320  13322252452.0  221.205.98.55      133  2225  2452
10  2308024342  18922257681.0     183.184.230.38  189  2225  7681
11  2308024310  19934210999.0     183.184.230.39  199  3421  0999
12  2308024435  19934210911.0     185.184.230.40  199  3421  0911
13  2308024432  19934210912.0     183.154.230.41  199  3421  0912
14  2308024446  19934210913.0     183.184.231.42  199  3421  0913
15  2308024421  19934210914.0     183.154.230.43  199  3421  0914
16  2308024433  19934210915.0     173.184.230.44  199  3421  0915
17  2308024428  19934210916.0      183.184.230.4  199  3421  0916
18  2308024402  19934210917.0      183.184.230.4  199  3421  0917
19  2308024422  19934210918.0      153.144.230.7  199  3421  0918
```
我们看到新表的最后创建的列名都是电话。 
我们应该修改这些列名,再和大表合并
我们可以看到brands的类型是object, name='电话'， 而该列名就是大表的列名。 所以我们修改name值，
就可以得到新的列名。

```Python

brands.name = '移动商'
areas.name = '地区'
tells.name = '号码'
new_data = pd.concat([my_data, brands, areas, tells], axis=1)
```

最后删除电话这一列

```Python
print(new_data.drop(['电话'],axis=1))
[output]
  学号                 IP  移动商    地区    号码
0   2308024241      221.205.98.55  189  2225  4812
1   2308024244    183.184.226.205  135  2225  5003
2   2308024251      221.205.98.55  134  2225  9938
3   2308024249      222.31.51.200  188  2225  6753
4   2308024219       120.207.64.3  189  2225  3721
5   2308024201      222.31.51.200  138  2225  4373
6   2308024347      222.31.59.220  138  2225  4373
7   2308024307  221.205.98.55      133  2225  2452
8   2308024326     183.184.230.38  189  2225  7681
9   2308024320  221.205.98.55      133  2225  2452
10  2308024342     183.184.230.38  189  2225  7681
11  2308024310     183.184.230.39  199  3421  0999
12  2308024435     185.184.230.40  199  3421  0911
13  2308024432     183.154.230.41  199  3421  0912
14  2308024446     183.184.231.42  199  3421  0913
15  2308024421     183.154.230.43  199  3421  0914
16  2308024433     173.184.230.44  199  3421  0915
17  2308024428      183.184.230.4  199  3421  0916
18  2308024402      183.184.230.4  199  3421  0917
19  2308024422      153.144.230.7  199  3421  0918

删除第一行为：
print(new_data.drop([0],axis=0))
```

### 3. 修改记录

1. 个别替换

df.replace('a','b')  # 把a用b进行替换

2. 批量替换

```Python
import pandas as pd
# 把作弊， 和缺考统统替换为0
df = pd.read_csv(r'd:\textfiles\rz.csv', delimiter=',')
df = df.replace(['作弊', '缺考'], 0)
print(df)
```

3. 特定替换

```Python
# replace({'列名'：'该列的某一单元内容','列名'：'该列的某一单元内容','列名'：'该列的某一单元内容'},需替换的内容)
```

4. 多值替换

```Python
df = df.replace(['作弊', '缺考'], ['偷看', '没来']) ([被替换的词语列表],[替换新的词语列表])
```



## 3.数据交换

### 1. 交换行

```Python
reindex=[1,2,0,3,5,7,9,11,13,15,17,19,4,6,8,10,12,14,16,18,20]
print(df)
df = df.reindex(reindex)
print(df)
```

### 2. 交换列


```Python
import pandas as pd
dt = pd.read_csv(r'k:\rz.csv', sep=',')


dt_name = dt['姓名']
dt_id = dt['学号']

dt = dt.drop(columns=['姓名', '学号'], axis=1)

dt.insert(0,dt_name.name,dt_name)# 三个参数 位置， 列名， 该列的具体内容 dt_name可以是元组或列表类型。
print(dt.head())
```

## 4. 数据计算

### 1. 数据合并

用 df_new = concat(df1,df2); 或者 df1.append(df2, ignore_index=True) 来进行追加。


### 2. 追加列

df['abc']#  abc为新列名
df['abc'] = df_new_list

@@@ 要注意的是： 如果列要做计算 需要将列的变量类型进行转换：  比如 df['name']=astype(string), df['age']=astype(int)

